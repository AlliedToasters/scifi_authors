{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "from scipy import spatial\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('docs.csv').sample(frac=1, random_state=0) #load dataset and shuffle rows\n",
    "df.index = range(0, 100) #reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Reserve holdout group\n",
    "train = df.loc[:74].copy() #train group\n",
    "test = df.loc[75:].copy() #holdout group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemmas(document):\n",
    "    \"\"\"takes raw spacy parse and returns only\n",
    "    word lemmas, in or out of vocab.\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    for token in document:\n",
    "        if not token.is_space and not token.is_punct and not (token.lemma_ == '-PRON-'):\n",
    "            result += token.lemma_ + ' '\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            result += token.orth_ + ' '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prs = spacy.load('en')\n",
    "train['raw_parse'] = train.text.apply(prs)\n",
    "train['lemmas'] = train.raw_parse.apply(get_lemmas)\n",
    "vec = TfidfVectorizer(stop_words='english', min_df=2, max_df=.99, ngram_range=(1, 3))\n",
    "svd = TruncatedSVD(n_components=74, random_state=0, algorithm='arpack')\n",
    "norm = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, norm)\n",
    "raw_vec_train = vec.fit_transform(train.lemmas)\n",
    "train_mat = lsa.fit_transform(raw_vec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode the authors and the source texts for cluster evaluation\n",
    "author_encoding = {}\n",
    "code = iter(range(0, len(df.author.unique())))\n",
    "for auth in df.author.unique():\n",
    "    author_encoding[auth] = next(code)\n",
    "    \n",
    "inverted_authcode = dict([[v,k] for k,v in author_encoding.items()])\n",
    "    \n",
    "train['author_code'] = train.author.apply(lambda x: author_encoding[x])\n",
    "test['author_code'] = test.author.apply(lambda x: author_encoding[x])\n",
    "\n",
    "title_encoding = {}\n",
    "code = iter(range(0, len(df.title.unique())))\n",
    "for tit in df.title.unique():\n",
    "    title_encoding[tit] = next(code)\n",
    "    \n",
    "inverted_titcode = dict([[v,k] for k,v in title_encoding.items()])\n",
    "    \n",
    "train['title_code'] = train.title.apply(lambda x: title_encoding[x])\n",
    "test['title_code'] = test.title.apply(lambda x: title_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = vec.fit_transform(train.lemmas)\n",
    "train_mat = lsa.fit_transform(trans)\n",
    "X = train_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['raw_parse'] = test.text.apply(prs)\n",
    "test['lemmas'] = test.raw_parse.apply(get_lemmas)\n",
    "trans = vec.transform(test.lemmas)\n",
    "test_mat = lsa.transform(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train_mat, label=train.author_code)\n",
    "dtest = xgb.DMatrix(test_mat, label=test.author_code)\n",
    "\n",
    "param = {\n",
    "    'max_depth': 7, \n",
    "    'eta': .001, \n",
    "    'silent': 1, \n",
    "    'objective': 'multi:softmax',\n",
    "    'nthread':4,\n",
    "    'eval_metric':'mlogloss',\n",
    "    'num_class': 10,\n",
    "    'colsample_bytree': .4\n",
    "    \n",
    "}\n",
    "\n",
    "plst = param.items()\n",
    "num_round = 11670\n",
    "evallist = [(dtest, 'eval'), (dtrain, 'train')]\n",
    "bst = xgb.train(plst, dtrain, num_round, evallist, verbose_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92000000000000004"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = bst.predict(dtest)\n",
    "accuracy_score(predictions, test.author_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "test['predicted'] = predictions.astype(int)\n",
    "test['predicted_author'] = test.predicted.apply(lambda x: inverted_authcode[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "      <th>date</th>\n",
       "      <th>fiction</th>\n",
       "      <th>length</th>\n",
       "      <th>author_code</th>\n",
       "      <th>title_code</th>\n",
       "      <th>raw_parse</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>predicted</th>\n",
       "      <th>predicted_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>\\nA SHIFTING REEF\\n\\nThe year 1866 was signali...</td>\n",
       "      <td>20000leagues</td>\n",
       "      <td>verne</td>\n",
       "      <td>1870</td>\n",
       "      <td>1</td>\n",
       "      <td>1146</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>(\\n, A, SHIFTING, REEF, \\n\\n, The, year, 1866,...</td>\n",
       "      <td>a shifting reef the year 1866 be signalise by ...</td>\n",
       "      <td>5</td>\n",
       "      <td>wells</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>\\nEverything was perfectly swell.\\n\\nThere wer...</td>\n",
       "      <td>2BR02B</td>\n",
       "      <td>vonnegut</td>\n",
       "      <td>1962</td>\n",
       "      <td>1</td>\n",
       "      <td>1152</td>\n",
       "      <td>1</td>\n",
       "      <td>19</td>\n",
       "      <td>(\\n, Everything, was, perfectly, swell, ., \\n\\...</td>\n",
       "      <td>everything be perfectly swell there be no pris...</td>\n",
       "      <td>7</td>\n",
       "      <td>herbert</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text         title    author  \\\n",
       "78  \\nA SHIFTING REEF\\n\\nThe year 1866 was signali...  20000leagues     verne   \n",
       "93  \\nEverything was perfectly swell.\\n\\nThere wer...        2BR02B  vonnegut   \n",
       "\n",
       "    date  fiction  length  author_code  title_code  \\\n",
       "78  1870        1    1146            4           4   \n",
       "93  1962        1    1152            1          19   \n",
       "\n",
       "                                            raw_parse  \\\n",
       "78  (\\n, A, SHIFTING, REEF, \\n\\n, The, year, 1866,...   \n",
       "93  (\\n, Everything, was, perfectly, swell, ., \\n\\...   \n",
       "\n",
       "                                               lemmas  predicted  \\\n",
       "78  a shifting reef the year 1866 be signalise by ...          5   \n",
       "93  everything be perfectly swell there be no pris...          7   \n",
       "\n",
       "   predicted_author  \n",
       "78            wells  \n",
       "93          herbert  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[test.predicted!=test.author_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting that the gradient boosted classifier mislabels the same example as the cluster analysis, but with a different author."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_dspy3)",
   "language": "python",
   "name": "conda_dspy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
