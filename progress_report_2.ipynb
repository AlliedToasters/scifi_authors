{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Progress Report 2\n",
    "With some inspiration from <a href='http://scikit-learn.org/stable/auto_examples/text/document_clustering.html#sphx-glr-auto-examples-text-document-clustering-py'>this sklearn text clustering tutorial</a>, I tried four different clustering algorithms and tested their performance in clustering by author using the adjusted rand index, as well as homogeniety, completeness, and V-score (harmonic of homogeniety and completeness). Affinity propogation outperforms all the others and ends up grouping almost perfectly by the source texts (each chapter comes from one of two works by each author.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import spacy\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.cluster import KMeans, MeanShift, SpectralClustering, AffinityPropagation\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "import gc\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('docs.csv').sample(frac=1, random_state=0) #shuffle rows\n",
    "df.index = range(0, 100) #reindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Encode the authors and the source texts\n",
    "\n",
    "author_encoding = {}\n",
    "code = iter(range(0, len(df.author.unique())))\n",
    "for auth in df.author.unique():\n",
    "    author_encoding[auth] = next(code)\n",
    "    \n",
    "df['author_code'] = df.author.apply(lambda x: author_encoding[x])\n",
    "\n",
    "title_encoding = {}\n",
    "code = iter(range(0, len(df.title.unique())))\n",
    "for tit in df.title.unique():\n",
    "    title_encoding[tit] = next(code)\n",
    "    \n",
    "df['title_code'] = df.title.apply(lambda x: title_encoding[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = df.loc[:74].copy() #train group\n",
    "test = df.loc[75:].copy() #holdout group\n",
    "len(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and process text as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prs = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['raw_parse'] = train.text.apply(prs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_lemmas(document):\n",
    "    \"\"\"takes raw spacy parse and returns only\n",
    "    word lemmas, in or out of vocab.\n",
    "    \"\"\"\n",
    "    result = ''\n",
    "    for token in document:\n",
    "        if not token.is_space and not token.is_punct and not (token.lemma_ == '-PRON-'):\n",
    "            result += token.lemma_ + ' '\n",
    "        elif token.lemma_ == '-PRON-':\n",
    "            result += token.orth_ + ' '\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train['lemmas'] = train.raw_parse.apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add normalization to LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(stop_words='english', min_df=2, max_df=.99, ngram_range=(1, 3))\n",
    "svd = TruncatedSVD(n_components=71, random_state=0, algorithm='arpack')\n",
    "norm = Normalizer(copy=False)\n",
    "lsa = make_pipeline(svd, norm)\n",
    "trans = vec.fit_transform(train.lemmas)\n",
    "train_mat = lsa.fit_transform(trans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KMeans, Mean Shift, Spectral Clustering, and Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:  19\n",
      "Homogeneity: 0.975\n",
      "Completeness: 0.760\n",
      "V-measure: 0.854\n",
      "Adjusted Rand-Index: 0.583\n",
      "Silhouette Coefficient: 0.172\n"
     ]
    }
   ],
   "source": [
    "#KMeans gets best ARI at 19 clusters\n",
    "km = KMeans(random_state=0, n_clusters=19)\n",
    "np.unique(km.fit_predict(train_mat))\n",
    "print('number of clusters: ', len(km.cluster_centers_))\n",
    "labels = train.author_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, km.labels_, sample_size=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:  50\n",
      "Homogeneity: 1.000\n",
      "Completeness: 0.605\n",
      "V-measure: 0.754\n",
      "Adjusted Rand-Index: 0.272\n",
      "Silhouette Coefficient: 0.128\n"
     ]
    }
   ],
   "source": [
    "#Mean shift does best at bandwidth=1.025, but not nearly as well as kmeans\n",
    "ms = MeanShift(bandwidth=1.025)\n",
    "np.unique(ms.fit_predict(train_mat))\n",
    "print('number of clusters: ', len(ms.cluster_centers_))\n",
    "labels = train.author_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ms.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ms.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ms.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ms.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, ms.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:  10\n",
      "Homogeneity: 0.941\n",
      "Completeness: 0.728\n",
      "V-measure: 0.821\n",
      "Adjusted Rand-Index: 0.520\n",
      "Silhouette Coefficient: 0.193\n"
     ]
    }
   ],
   "source": [
    "#Spectral clustering does best with 20 clusters, suggesting grouping by 20 unique source texts.\n",
    "sc = SpectralClustering(n_clusters=20)\n",
    "np.unique(sc.fit_predict(train_mat))\n",
    "print('number of clusters: ', n_clusters)\n",
    "labels = train.author_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, sc.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, sc.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, sc.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, sc.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, sc.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.939\n",
      "Completeness: 0.937\n",
      "V-measure: 0.938\n",
      "Adjusted Rand-Index: 0.804\n",
      "Silhouette Coefficient: 0.193\n"
     ]
    }
   ],
   "source": [
    "#Sure enough, we see high correlation with the source texts.\n",
    "labels = train.title_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, sc.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, sc.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, sc.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, sc.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, sc.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:  19\n",
      "Homogeneity: 0.989\n",
      "Completeness: 0.768\n",
      "V-measure: 0.865\n",
      "Adjusted Rand-Index: 0.592\n",
      "Silhouette Coefficient: 0.187\n"
     ]
    }
   ],
   "source": [
    "#Finally, affinity propogation \"finds\" 19 clusters without any supervision.\n",
    "ap = AffinityPropagation()\n",
    "np.unique(ap.fit_predict(train_mat))\n",
    "print('number of clusters: ', len(ap.cluster_centers_))\n",
    "labels = train.author_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ap.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ap.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ap.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ap.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, ap.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homogeneity: 0.967\n",
      "Completeness: 0.970\n",
      "V-measure: 0.968\n",
      "Adjusted Rand-Index: 0.901\n",
      "Silhouette Coefficient: 0.189\n"
     ]
    }
   ],
   "source": [
    "#The 19 clusters line up really well with the 20 source texts, with the highest ARI we've seen yet.\n",
    "labels = train.title_code\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ap.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ap.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ap.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ap.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, ap.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of clusters:  19\n",
      "Homogeneity: 0.976\n",
      "Completeness: 0.981\n",
      "V-measure: 0.978\n",
      "Adjusted Rand-Index: 0.927\n",
      "Silhouette Coefficient: 0.182\n"
     ]
    }
   ],
   "source": [
    "#Improve performance further by adjusting LSA hyperparameters.\n",
    "svd.n_components = 74\n",
    "svd.algorithm = 'arpack'\n",
    "trans = vec.fit_transform(train.lemmas)\n",
    "train_mat = lsa.fit_transform(trans)\n",
    "ap = AffinityPropagation()\n",
    "np.unique(ap.fit_predict(train_mat))\n",
    "labels = train.title_code\n",
    "print('number of clusters: ', len(ap.cluster_centers_))\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, ap.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, ap.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, ap.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, ap.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(train_mat, ap.labels_, sample_size=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_dspy3)",
   "language": "python",
   "name": "conda_dspy3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
